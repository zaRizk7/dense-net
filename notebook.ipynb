{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from densenet import DenseNet\n",
    "\n",
    "seed = datetime.today().year\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, metadata: pd.DataFrame, transform=None):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata.reset_index(drop=True)\n",
    "        self.classes_to_idx = {\n",
    "            cls: idx for idx, cls in enumerate(self.metadata.target.unique())\n",
    "        }\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        metadata = self.metadata.iloc[idx]\n",
    "        image = Image.open(metadata.image)\n",
    "        image = np.asarray(image.convert(\"RGB\"))\n",
    "        label = self.classes_to_idx[metadata.target]\n",
    "        if not self.transform is None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def create_metadata(root: str) -> pd.DataFrame:\n",
    "    classes = sorted([folder for folder in os.listdir(root)])\n",
    "    metadata = []\n",
    "    for cls in classes:\n",
    "        subfolder = f\"{root}/{cls}\"\n",
    "        for image in os.listdir(subfolder):\n",
    "            metadata += [{\"image\": f\"{subfolder}/{image}\", \"target\": cls}]\n",
    "    metadata = pd.DataFrame(metadata)\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "metadata_train = create_metadata(\"train\")\n",
    "metadata_valid = create_metadata(\"valid\")\n",
    "metadata_test = create_metadata(\"test\")\n",
    "\n",
    "transform_train = A.Compose(\n",
    "    [\n",
    "        A.VerticalFlip(p=1 / 16),\n",
    "        A.HorizontalFlip(p=1 / 16),\n",
    "        A.ColorJitter(p=1 / 16),\n",
    "        A.Affine(1.25, p=1 / 16),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_eval = A.Compose([A.Resize(224, 224), A.Normalize()])\n",
    "\n",
    "dataset_train = ImageDataset(metadata_train, transform_train)\n",
    "dataset_valid = ImageDataset(metadata_valid, transform_eval)\n",
    "dataset_test = ImageDataset(metadata_test, transform_eval)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(dataset_train, batch_size, True)\n",
    "validloader = DataLoader(dataset_valid, batch_size)\n",
    "testloader = DataLoader(dataset_test, batch_size)\n",
    "\n",
    "model = DenseNet(len(dataset_train.classes_to_idx), 3).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, len(trainloader) * epochs // 2\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "loss_function_train = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "loss_function_eval = torch.nn.CrossEntropyLoss(reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1825/1825 [09:26<00:00,  3.22it/s, status=Done!, loss=3.88, accuracy=0.0828, precision=0.0456, recall=0.0487, f1=0.0461, val_loss=2.35, val_accuracy=0.243, val_precision=0.24, val_recall=0.244, val_f1=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1825/1825 [09:21<00:00,  3.25it/s, status=Done!, loss=2.83, accuracy=0.387, precision=0.252, recall=0.252, f1=0.25, val_loss=1.56, val_accuracy=0.539, val_precision=0.636, val_recall=0.539, val_f1=0.523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1825/1825 [09:36<00:00,  3.17it/s, status=Done!, loss=2.57, accuracy=0.576, precision=0.417, recall=0.416, f1=0.414, val_loss=0.95, val_accuracy=0.725, val_precision=0.774, val_recall=0.725, val_f1=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 538/1825 [02:52<06:52,  3.12it/s, status=Terminated!, loss=2.76, accuracy=0.666, precision=0.512, recall=0.51, f1=0.509, val_loss=nan, val_accuracy=0.0025, val_precision=6.25e-6, val_recall=0.0025, val_f1=1.25e-5]\n"
     ]
    }
   ],
   "source": [
    "loss_best = torch.inf\n",
    "history = []\n",
    "for i in range(1, epochs + 1):\n",
    "    print(f\"Epoch {i}/{epochs}\")\n",
    "    progbar = tqdm(total=len(trainloader))\n",
    "    model.train()\n",
    "    metrics_train = defaultdict(float)\n",
    "    results_train = defaultdict(list)\n",
    "    metrics_train[\"status\"] = \"Training\"\n",
    "    for i, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function_train(outputs, targets)\n",
    "\n",
    "        if loss.isnan():\n",
    "            metrics_train[\"status\"] = \"Terminated!\"\n",
    "            break\n",
    "\n",
    "        scale = scaler.get_scale()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.step()\n",
    "        scaler.update()\n",
    "        if scale > scaler.get_scale():\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        outputs = outputs.detach().cpu().argmax(-1).numpy()\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "\n",
    "        score = classification_report(\n",
    "            targets, outputs, output_dict=True, zero_division=0\n",
    "        )\n",
    "\n",
    "        results_train[\"loss\"] += [loss]\n",
    "        results_train[\"accuracy\"] += [score[\"accuracy\"]]\n",
    "        results_train[\"precision\"] += [score[\"macro avg\"][\"precision\"]]\n",
    "        results_train[\"recall\"] += [score[\"macro avg\"][\"recall\"]]\n",
    "        results_train[\"f1\"] += [score[\"macro avg\"][\"f1-score\"]]\n",
    "\n",
    "        metrics_train[\"loss\"] = np.mean(loss)\n",
    "        metrics_train[\"accuracy\"] = np.mean(results_train[\"accuracy\"])\n",
    "        metrics_train[\"precision\"] = np.mean(results_train[\"precision\"])\n",
    "        metrics_train[\"recall\"] = np.mean(results_train[\"recall\"])\n",
    "        metrics_train[\"f1\"] = np.mean(results_train[\"f1\"])\n",
    "\n",
    "        progbar.set_postfix(metrics_train)\n",
    "        progbar.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    metrics_valid = defaultdict(float)\n",
    "    results_valid = defaultdict(list)\n",
    "    for i, (inputs, targets) in enumerate(validloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function_eval(outputs, targets)\n",
    "\n",
    "        loss = loss.detach().cpu().numpy().tolist()\n",
    "        outputs = outputs.detach().cpu().argmax(-1).numpy().tolist()\n",
    "        targets = targets.detach().cpu().numpy().tolist()\n",
    "\n",
    "        results_valid[\"loss\"] += loss\n",
    "        results_valid[\"output\"] += outputs\n",
    "        results_valid[\"target\"] += targets\n",
    "\n",
    "    score = classification_report(\n",
    "        results_valid[\"target\"],\n",
    "        results_valid[\"output\"],\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    metrics_valid[\"val_loss\"] = np.mean(loss)\n",
    "    metrics_valid[\"val_accuracy\"] = score[\"accuracy\"]\n",
    "    metrics_valid[\"val_precision\"] = score[\"macro avg\"][\"precision\"]\n",
    "    metrics_valid[\"val_recall\"] = score[\"macro avg\"][\"recall\"]\n",
    "    metrics_valid[\"val_f1\"] = score[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    metrics = {**metrics_train, **metrics_valid}\n",
    "    if metrics[\"status\"] == \"Training\":\n",
    "        metrics[\"status\"] = \"Done!\"\n",
    "    progbar.set_postfix(metrics)\n",
    "    progbar.close()\n",
    "    history += [metrics]\n",
    "    if loss_best > history[-1][\"val_loss\"]:\n",
    "        loss_best = history[-1][\"val_loss\"]\n",
    "        model_best = deepcopy(model.state_dict())\n",
    "        optimizer_best = deepcopy(optimizer.state_dict())\n",
    "        scheduler_best = deepcopy(scheduler.state_dict())\n",
    "        scaler_best = deepcopy(scaler.state_dict())\n",
    "\n",
    "    model.load_state_dict(model_best)\n",
    "    optimizer.load_state_dict(optimizer_best)\n",
    "    scheduler.load_state_dict(scheduler_best)\n",
    "    scaler.load_state_dict(scaler_best)\n",
    "\n",
    "    if metrics[\"status\"] == \"Terminated!\" or np.isnan(metrics[\"val_loss\"]):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:06<00:00, 10.20it/s, loss=0.671, accuracy=0.762, precision=0.817, recall=0.762, f1=0.757]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "metrics_test = defaultdict(float)\n",
    "results_test = defaultdict(list)\n",
    "progbar = tqdm(total=len(testloader))\n",
    "for i, (inputs, targets) in enumerate(testloader):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function_eval(outputs, targets)\n",
    "\n",
    "    loss = loss.detach().cpu().numpy().tolist()\n",
    "    outputs = outputs.detach().cpu().argmax(-1).numpy().tolist()\n",
    "    targets = targets.detach().cpu().numpy().tolist()\n",
    "\n",
    "    results_test[\"loss\"] += loss\n",
    "    results_test[\"output\"] += outputs\n",
    "    results_test[\"target\"] += targets\n",
    "\n",
    "    score = classification_report(\n",
    "        results_test[\"target\"],\n",
    "        results_test[\"output\"],\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    metrics_test[\"loss\"] = np.mean(loss)\n",
    "    metrics_test[\"accuracy\"] = score[\"accuracy\"]\n",
    "    metrics_test[\"precision\"] = score[\"macro avg\"][\"precision\"]\n",
    "    metrics_test[\"recall\"] = score[\"macro avg\"][\"recall\"]\n",
    "    metrics_test[\"f1\"] = score[\"macro avg\"][\"f1-score\"]\n",
    "    progbar.set_postfix(metrics_test)\n",
    "    progbar.update(1)\n",
    "metrics_test = {k: v / (i + 1) for k, v in metrics_test.items()}\n",
    "progbar.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
